net: EfficientNetV2

training:
  num_epochs: 30
  patience: 5
  lr: 0.01
  scheduler:
    step_size: 7
    gamma: 0.1
  frozen_layers: 2
  optimizer:
    momentum: 0.9
    weight_decay: 0.0001

data:
  batch_size: 32
