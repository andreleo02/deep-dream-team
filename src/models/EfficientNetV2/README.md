The advent of residual neural networks (ResNets) represented a significant development in the field of deep learning, as they were the first architectures to address the crucial problem of vanishing gradients. The core concept behind ResNets is the introduction of "residual blocks" in the network, which enable the network to focus on learning residuals (the difference between the input and the output) instead of attempting to learn a direct mapping from the input to the output. Formally, if the desired underlying mapping is \( H(x) \), ResNet reformulates it as \( H(x) = F(x) + x \), where \( F(x) \) represents the residual mapping that the network learns. 
