net: SwinTransformer

training:
  num_epochs: 30
  patience: 5
  lr: 0.05
  scheduler:
    step_size: 7
    gamma: 0.1
  frozen_layers: 3
  optimizer:
    momentum: 0.9

data:
  batch_size: 32
