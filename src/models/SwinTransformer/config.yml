net: SwinTransformer

training:
  num_epochs: 20
  patience: 6
  lr: 0.01
  scheduler:
    step_size: 20
    gamma: 0.1
  frozen_layers: 4
  optimizer:
    momentum: 0.9

data:
  batch_size: 32
