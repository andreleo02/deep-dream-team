net: SwinTransformer

training:
  num_epochs: 20
  patience: 5
  lr: 0.001
  scheduler:
    step_size: 10
    gamma: 0.1
  frozen_layers: 3
  optimizer:
    momentum: 0.9

data:
  batch_size: 32
